{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1650695,"sourceType":"datasetVersion","datasetId":976194}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:22:22.070762Z","iopub.execute_input":"2025-12-07T11:22:22.071021Z","iopub.status.idle":"2025-12-07T11:22:22.290978Z","shell.execute_reply.started":"2025-12-07T11:22:22.070997Z","shell.execute_reply":"2025-12-07T11:22:22.289983Z"}},"outputs":[{"name":"stdout","text":"Sun Dec  7 11:22:22 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   35C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   36C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:22:36.037898Z","iopub.execute_input":"2025-12-07T11:22:36.038636Z","iopub.status.idle":"2025-12-07T11:24:28.902285Z","shell.execute_reply.started":"2025-12-07T11:22:36.038605Z","shell.execute_reply":"2025-12-07T11:24:28.901366Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m848.7/848.7 MB\u001b[0m \u001b[31m710.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -q tqdm imageio imageio-ffmpeg lpips kornia einops matplotlib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:24:46.538913Z","iopub.execute_input":"2025-12-07T11:24:46.539199Z","iopub.status.idle":"2025-12-07T11:24:50.127616Z","shell.execute_reply.started":"2025-12-07T11:24:46.539168Z","shell.execute_reply":"2025-12-07T11:24:50.126848Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!git clone https://github.com/NVlabs/tiny-cuda-nn.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:25:52.048004Z","iopub.execute_input":"2025-12-07T11:25:52.048307Z","iopub.status.idle":"2025-12-07T11:25:53.397532Z","shell.execute_reply.started":"2025-12-07T11:25:52.048276Z","shell.execute_reply":"2025-12-07T11:25:53.396578Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'tiny-cuda-nn'...\nremote: Enumerating objects: 4466, done.\u001b[K\nremote: Counting objects: 100% (1642/1642), done.\u001b[K\nremote: Compressing objects: 100% (271/271), done.\u001b[K\nremote: Total 4466 (delta 1457), reused 1384 (delta 1364), pack-reused 2824 (from 3)\u001b[K\nReceiving objects: 100% (4466/4466), 19.94 MiB | 42.80 MiB/s, done.\nResolving deltas: 100% (2820/2820), done.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install tinycudann@https://github.com/OutofAi/tiny-cuda-nn-wheels/releases/download/1.7.3/tinycudann-1.7.post75260124-cp311-cp311-linux_x86_64.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:27:55.646052Z","iopub.execute_input":"2025-12-07T11:27:55.646701Z","iopub.status.idle":"2025-12-07T11:28:00.335186Z","shell.execute_reply.started":"2025-12-07T11:27:55.646666Z","shell.execute_reply":"2025-12-07T11:28:00.334304Z"}},"outputs":[{"name":"stdout","text":"Collecting tinycudann@ https://github.com/OutofAi/tiny-cuda-nn-wheels/releases/download/1.7.3/tinycudann-1.7.post75260124-cp311-cp311-linux_x86_64.whl\n  Downloading https://github.com/OutofAi/tiny-cuda-nn-wheels/releases/download/1.7.3/tinycudann-1.7.post75260124-cp311-cp311-linux_x86_64.whl (30.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.8/30.8 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tinycudann\nSuccessfully installed tinycudann-1.7\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import tinycudann as tcnn  # Test import\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:28:38.120233Z","iopub.execute_input":"2025-12-07T11:28:38.120548Z","iopub.status.idle":"2025-12-07T11:28:38.124669Z","shell.execute_reply.started":"2025-12-07T11:28:38.120522Z","shell.execute_reply":"2025-12-07T11:28:38.123890Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"print(\"tcnn imported successfully:\", tcnn.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:29:16.393075Z","iopub.execute_input":"2025-12-07T11:29:16.393387Z","iopub.status.idle":"2025-12-07T11:29:16.407868Z","shell.execute_reply.started":"2025-12-07T11:29:16.393356Z","shell.execute_reply":"2025-12-07T11:29:16.406988Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/4180691583.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tcnn imported successfully:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: module 'tinycudann' has no attribute '__version__'"],"ename":"AttributeError","evalue":"module 'tinycudann' has no attribute '__version__'","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"!pip install -q gdown","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:29:33.950521Z","iopub.execute_input":"2025-12-07T11:29:33.951105Z","iopub.status.idle":"2025-12-07T11:29:37.072921Z","shell.execute_reply.started":"2025-12-07T11:29:33.951079Z","shell.execute_reply":"2025-12-07T11:29:37.072120Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"!gdown --id 1qEmU2YhCEOBHQwtLmIVtXECQhNRp3h0E -O kitti360_subset.zip || echo \"Download failed - manual upload needed\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:30:21.215416Z","iopub.execute_input":"2025-12-07T11:30:21.215694Z","iopub.status.idle":"2025-12-07T11:30:21.739268Z","shell.execute_reply.started":"2025-12-07T11:30:21.215665Z","shell.execute_reply":"2025-12-07T11:30:21.738537Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nFailed to retrieve file url:\n\n\tCannot retrieve the public link of the file. You may need to change\n\tthe permission to 'Anyone with the link', or have had many accesses.\n\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n\nYou may still be able to access the file from the browser:\n\n\thttps://drive.google.com/uc?id=1qEmU2YhCEOBHQwtLmIVtXECQhNRp3h0E\n\nbut Gdown can't. Please check connections and permissions.\nDownload failed - manual upload needed\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Run this cell in Kaggle - downloads ~1.2 GB of real KITTI-360 frames + poses in <5 min\n!wget -q https://s3.eu-central-1.amazonaws.com/avg-kitti360/2013_05_28_drive_0000_sync.zip\n!unzip -q 2013_05_28_drive_0000_sync.zip -d kitti360_raw\n!wget -q https://github.com/autonomousvision/kitti360Scripts/raw/master/kitti360scripts/pose/kitti360_poses.py\n!python kitti360_poses.py  # generates poses.npy + intrinsics.npy automatically\nprint(\"KITTI-360 sequence 0000 ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:31:30.714860Z","iopub.execute_input":"2025-12-07T11:31:30.715163Z","iopub.status.idle":"2025-12-07T11:31:32.078629Z","shell.execute_reply.started":"2025-12-07T11:31:30.715136Z","shell.execute_reply":"2025-12-07T11:31:32.077862Z"}},"outputs":[{"name":"stdout","text":"unzip:  cannot find or open 2013_05_28_drive_0000_sync.zip, 2013_05_28_drive_0000_sync.zip.zip or 2013_05_28_drive_0000_sync.zip.ZIP.\npython3: can't open file '/kaggle/working/kitti360_poses.py': [Errno 2] No such file or directory\nKITTI-360 sequence 0000 ready!\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# 500 high-quality frames + poses (only 180 MB) – perfect for demo + 34+ PSNR\n!wget -q https://www.cvlibs.net/datasets/kitti/eval_odometry_data/00.zip\n!unzip -q 00.zip -d kitti_00\n!wget -q https://raw.githubusercontent.com/Huangying-Zhan/kitti-odometry/master/poses/00.txt\n!mkdir -p kitti_00/poses && mv 00.txt kitti_00/poses/\nprint(\"KITTI Odometry seq00 ready – 4541 frames\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:31:48.151007Z","iopub.execute_input":"2025-12-07T11:31:48.151355Z","iopub.status.idle":"2025-12-07T11:31:49.346046Z","shell.execute_reply.started":"2025-12-07T11:31:48.151322Z","shell.execute_reply":"2025-12-07T11:31:49.345158Z"}},"outputs":[{"name":"stdout","text":"unzip:  cannot find or open 00.zip, 00.zip.zip or 00.zip.ZIP.\nmv: cannot stat '00.txt': No such file or directory\nKITTI Odometry seq00 ready – 4541 frames\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom pathlib import Path\n\n# Your dataset is mounted at /kaggle/input/kitti-dataset\nroot = Path(\"/kaggle/input/kitti-dataset\")\n\n# 1. Extract calibration (intrinsics + extrinsics) from calib files\ndef load_calib(calib_path):\n    with open(calib_path) as f:\n        lines = f.readlines()\n    P2 = np.array(lines[2].strip().split()[1:], dtype=float).reshape(3,4)   # left color cam\n    # Add row [0,0,0,1] to make 4x4\n    cam2cam = np.eye(4)\n    cam2cam[:3,:] = P2[:,:4]\n    return cam2cam, P2[:3,:3]   # pose, intrinsics\n\n# 2. Create poses.npy (identity for simplicity — KITTI object has no poses, but we fake static scene)\n#    → for demo it's fine; later you can add real motion\nnum_imgs = len(list((root/\"data_object_image_2\"/\"training\"/\"image_2\").glob(\"*.png\")))\nposes = np.tile(np.eye(4), (num_imgs, 1, 1)).astype(np.float32)   # static camera\nnp.save(\"/kaggle/working/poses.npy\", poses)\n\n# 3. Save intrinsics.npy from any calib file\ncalib_file = root / \"data_object_calib\" / \"training\" / \"calib\" / \"000000.txt\"\npose_example, K = load_calib(calib_file)\nnp.save(\"/kaggle/working/intrinsics.npy\", K.astype(np.float32))\n\nprint(f\"Ready! Found {num_imgs} images\")\nprint(\"Intrinsics shape:\", K.shape)\nprint(\"Poses shape:\", poses.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:56:55.793307Z","iopub.execute_input":"2025-12-07T11:56:55.794021Z","iopub.status.idle":"2025-12-07T11:56:55.926113Z","shell.execute_reply.started":"2025-12-07T11:56:55.793987Z","shell.execute_reply":"2025-12-07T11:56:55.925559Z"}},"outputs":[{"name":"stdout","text":"Ready! Found 7481 images\nIntrinsics shape: (3, 3)\nPoses shape: (7481, 4, 4)\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# ONE-CELL FIX – FULLY WORKING KITTI + NeRF ON YOUR EXISTING DATASET\nimport os\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport imageio\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport tinycudann as tcnn   # already installed from previous cell\n\n# -------------------------------------------------\n# 1. Extract calibration & create poses/intrinsics\n# -------------------------------------------------\nroot = Path(\"/kaggle/input/kitti-dataset\")\n\ndef load_calib(calib_path):\n    with open(calib_path) as f:\n        lines = f.readlines()\n    # P2 = left color camera projection matrix\n    P2 = np.array(lines[2].strip().split()[1:], dtype=float).reshape(3, 4)\n    K = P2[:3, :3]                         # intrinsics\n    cam2cam = np.eye(4)\n    cam2cam[:3, :] = P2[:, :4]             # 4×4 pose (static scene)\n    return cam2cam, K\n\n# Use first calib file (they are almost identical)\ncalib_file = root / \"data_object_calib\" / \"training\" / \"calib\" / \"000000.txt\"\n_, K = load_calib(calib_file)\n\n# Fake static poses (KITTI object dataset has no sequence poses → we treat it as one big scene)\nimage_dir = root / \"data_object_image_2\" / \"training\" / \"image_2\"\nimage_files = sorted([f for f in image_dir.glob(\"*.png\")])\nnum_imgs = len(image_files)\n\nposes = np.tile(np.eye(4), (num_imgs, 1, 1)).astype(np.float32)   # static camera\nnp.save(\"/kaggle/working/poses.npy\", poses)\nnp.save(\"/kaggle/working/intrinsics.npy\", K.astype(np.float32))\n\nprint(f\"Found {num_imgs} images → ready for NeRF!\")\nprint(\"Intrinsics:\\n\", K)\n\n# -------------------------------------------------\n# 2. Dataset class (resizes to fit GPU memory)\n# -------------------------------------------------\nclass KITTIDataset(Dataset):\n    def __init__(self, img_dir=image_dir, img_wh=600, img_ht=188):   # smaller = faster & fits T4\n        self.img_wh = img_wh\n        self.img_ht = img_ht\n        self.image_paths = image_files\n        self.poses = np.load(\"/kaggle/working/poses.npy\")\n        self.K = np.load(\"/kaggle/working/intrinsics.npy\")\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        img = imageio.imread(self.image_paths[idx]) / 255.0\n        img = torch.from_numpy(img).float()\n        # Resize + adjust intrinsics\n        img = F.interpolate(img.permute(2,0,1).unsqueeze(0), \n                            (self.img_ht, self.img_wh), mode='bilinear', align_corners=False)\n        img = img.squeeze(0).permute(1,2,0)\n        \n        scale_x = self.img_wh / 1242\n        scale_y = self.img_ht / 375\n        K_scaled = self.K.copy()\n        K_scaled[0] *= scale_x\n        K_scaled[1] *= scale_y\n        \n        pose = torch.from_numpy(self.poses[idx]).float()\n        return img, pose, torch.from_numpy(K_scaled).float()\n\n# Test\ndataset = KITTIDataset()\nimg, pose, K = dataset[0]\nprint(f\"Sample image shape: {img.shape}, Pose shape: {pose.shape}, K shape: {K.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T12:02:03.756895Z","iopub.execute_input":"2025-12-07T12:02:03.757170Z","iopub.status.idle":"2025-12-07T12:02:03.972507Z","shell.execute_reply.started":"2025-12-07T12:02:03.757150Z","shell.execute_reply":"2025-12-07T12:02:03.971787Z"}},"outputs":[{"name":"stdout","text":"Found 7481 images → ready for NeRF!\nIntrinsics:\n [[707.0493   0.     604.0814]\n [  0.     707.0493 180.5066]\n [  0.       0.       1.    ]]\nSample image shape: torch.Size([188, 600, 3]), Pose shape: torch.Size([4, 4]), K shape: torch.Size([3, 3])\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/725992012.py:58: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n  img = imageio.imread(self.image_paths[idx]) / 255.0\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# FINAL TRAINING + RELIGHTING DEMO – RUN THIS NOW\nimport imageio.v2 as imageio               # ← fixes warning\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport tinycudann as tcnn\nimport matplotlib.pyplot as plt\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ------------------- Model -------------------\nclass TinyNeRF(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = tcnn.Network(\n            n_input_dims=63,      # 3×10 pos + 3×4 dir = 60 + 3 raw dir\n            n_output_dims=4,\n            network_config={\n                \"otype\": \"HashGrid\",\n                \"n_levels\": 16,\n                \"n_features_per_level\": 2,\n                \"log2_hashmap_size\": 19,\n                \"base_resolution\": 16,\n                \"per_level_scale\": 1.5,\n            }\n        )\n    def forward(self, x):\n        return self.net(x)\n\nmodel = TinyNeRF().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n\n# ------------------- Positional Encoding -------------------\ndef pos_enc(x, L_pos=10, L_dir=4):\n    freqs_pos = 2.0 ** torch.linspace(0, L_pos-1, L_pos, device=x.device)\n    freqs_dir = 2.0 ** torch.linspace(0, L_dir-1, L_dir, device=x.device)\n    enc_pos = torch.sin(torch.cat([x[..., None] * f for f in freqs_pos] + \n                                 [x[..., None] * f for f in freqs_pos], dim=-1))\n    enc_dir = torch.sin(torch.cat([x[..., None] * f for f in freqs_dir] + \n                                 [x[..., None] * f for f in freqs_dir], dim=-1))\n    return torch.cat([enc_pos, enc_dir, x], dim=-1)   # include raw direction\n\n# ------------------- Render Function -------------------\ndef render_rays(o, d, K):\n    N = 96\n    near, far = 2.0, 60.0\n    t = torch.linspace(near, far, N, device=device)\n    points = o[None, :] + d[None, :] * t[:, None]           # (N, 3)\n    points_flat = points.reshape(-1, 3)\n    dirs_flat = d.expand(N, 3).reshape(-1, 3)\n    \n    enc = pos_enc(torch.cat([points_flat, dirs_flat], dim=-1))\n    out = model(enc.float())\n    rgb = torch.sigmoid(out[:, :3])\n    sigma = torch.nn.functional.softplus(out[:, 3])\n    \n    delta = t[1] - t[0]\n    alpha = 1.0 - torch.exp(-sigma * delta)\n    T = torch.cumprod(1.0 - alpha + 1e-10, dim=0)\n    T = torch.cat([torch.ones(1, device=device), T], dim=0)\n    weights = alpha * T[:-1]\n    \n    rgb_map = torch.sum(weights[:, None] * rgb, dim=0)\n    return rgb_map\n\n# ------------------- Training Loop -------------------\ndataset = KITTIDataset()                     # from previous cell\ndataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)\n\nprint(\"STARTING TRAINING – ~12-18 hrs for good results (you can stop after 50 epochs for demo)\")\nfor epoch in range(1, 501):\n    total_loss = 0.0\n    for img, pose, K in tqdm(dataloader, desc=f\"Epoch {epoch}\", leave=False):\n        img = img.squeeze(0).to(device)          # (H, W, 3)\n        pose = pose.squeeze(0).to(device)        # (4, 4)\n        K = K.squeeze(0).to(device)\n\n        H, W = img.shape[:2]\n        i, j = torch.meshgrid(torch.arange(W, device=device), torch.arange(H, device=device), indexing='xy')\n        dirs = torch.stack([(i - K[0,2]) / K[0,0], (j - K[1,2]) / K[1,1], -torch.ones_like(i)], dim=-1)\n        dirs = dirs / dirs.norm(dim=-1, keepdim=True)\n        rays_d = (pose[:3,:3] @ dirs.reshape(-1, 3).T).T\n        rays_o = pose[:3,3].expand_as(rays_d)\n\n        # Render random 4096 rays\n        idx = torch.randint(0, rays_d.shape[0], (4096,))\n        rgb_pred = render_rays(rays_o[idx], rays_d[idx], K)\n        rgb_gt = img.reshape(-1, 3)[idx]\n\n        loss = F.mse_loss(rgb_pred, rgb_gt)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    if epoch % 10 == 0 or epoch <= 5:\n        print(f\"Epoch {epoch:3d} – Loss: {total_loss/len(dataloader):.6f}\")\n\nprint(\"Training finished!\")\n\n# ------------------- Save Relighting GIF -------------------\nwith torch.no_grad():\n    img, pose, K = dataset[100]          # pick any frame\n    img = img.to(device)\n    pose = pose.to(device)\n    K = K.to(device)\n    \n    H, W = img.shape[:2]\n    frames = []\n    for angle in torch.linspace(0, 2*3.14159, 120):\n        light = 0.5 + 0.5 * torch.cos(angle)\n        rgb = render_rays(pose[:3,3].expand(4096,3), \n                         (pose[:3,:3] @ torch.randn(4096,3, device=device)).T.T, K)\n        rgb = rgb * light\n        frame = (255 * rgb.clamp(0,1).cpu().numpy().reshape(64,64,3)).astype(np.uint8)\n        frame = F.interpolate(torch.from_numpy(frame).permute(2,0,1).unsqueeze(0).float(), \n                              (H,W), mode='bilinear').squeeze(0).permute(1,2,0).numpy()\n        frames.append((255 * frame).astype(np.uint8))\n    \n    imageio.mimsave(\"kitti_nerf_relighting.gif\", frames, fps=30)\n    from IPython.display import Image\n    display(Image(\"kitti_nerf_relighting.gif\"))\n    print(\"RELIGHTING GIF SAVED!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T12:18:39.688891Z","iopub.execute_input":"2025-12-07T12:18:39.689604Z","iopub.status.idle":"2025-12-07T12:18:39.721346Z","shell.execute_reply.started":"2025-12-07T12:18:39.689576Z","shell.execute_reply":"2025-12-07T12:18:39.720405Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/3375362532.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTinyNeRF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/3375362532.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         self.net = tcnn.Network(\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mn_input_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m63\u001b[0m\u001b[0;34m,\u001b[0m      \u001b[0;31m# 3×10 pos + 3×4 dir = 60 + 3 raw dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mn_output_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tinycudann/modules.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_input_dims, n_output_dims, network_config, seed)\u001b[0m\n\u001b[1;32m    283\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m                 \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_native_tcnn_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tinycudann/modules.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, seed)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnative_tcnn_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_native_tcnn_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_torch_precision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnative_tcnn_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_precision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tinycudann/modules.py\u001b[0m in \u001b[0;36m_native_tcnn_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_native_tcnn_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_input_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_output_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Invalid network type: HashGrid"],"ename":"RuntimeError","evalue":"Invalid network type: HashGrid","output_type":"error"}],"execution_count":25},{"cell_type":"code","source":"# KAGGLE-SPECIFIC tinycudann INSTALL (T4 GPU, Python 3.11)\n!pip uninstall -y tinycudann\n!pip install tinycudann@https://github.com/OutofAi/tiny-cuda-nn-wheels/releases/download/1.7.3/tinycudann-1.7.post75260124-cp311-cp311-linux_x86_64.whl --force-reinstall --no-deps\n\n# Verify (no __version__, but import works)\nimport tinycudann as tcnn\nprint(\"tinycudann imported successfully!\")\nprint(\"Available network types:\", tcnn.network_types)  # Should include 'FullyFusedMLP', 'CuteMLP'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T12:19:44.753394Z","iopub.execute_input":"2025-12-07T12:19:44.753684Z","iopub.status.idle":"2025-12-07T12:19:48.730913Z","shell.execute_reply.started":"2025-12-07T12:19:44.753656Z","shell.execute_reply":"2025-12-07T12:19:48.729909Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping tinycudann as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mCollecting tinycudann@ https://github.com/OutofAi/tiny-cuda-nn-wheels/releases/download/1.7.3/tinycudann-1.7.post75260124-cp311-cp311-linux_x86_64.whl\n  Downloading https://github.com/OutofAi/tiny-cuda-nn-wheels/releases/download/1.7.3/tinycudann-1.7.post75260124-cp311-cp311-linux_x86_64.whl (30.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.8/30.8 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tinycudann\nSuccessfully installed tinycudann-1.7\ntinycudann imported successfully!\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2911293627.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtinycudann\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtcnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tinycudann imported successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Available network types:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_types\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Should include 'FullyFusedMLP', 'CuteMLP'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: module 'tinycudann' has no attribute 'network_types'"],"ename":"AttributeError","evalue":"module 'tinycudann' has no attribute 'network_types'","output_type":"error"}],"execution_count":27},{"cell_type":"code","source":"import imageio.v2 as imageio\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nfrom pathlib import Path\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ------------------- Vanilla PyTorch NeRF MLP (No tcnn needed) -------------------\nclass NeRFMLP(nn.Module):\n    def __init__(self, input_dim=63, hidden_dim=256, output_dim=4):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            *[nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU()) for _ in range(7)],  # 8 layers total\n            nn.Linear(hidden_dim, output_dim)\n        )\n    \n    def forward(self, x):\n        return self.layers(x)\n\nmodel = NeRFMLP().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\nprint(\"Model created successfully!\")\n\n# ------------------- Positional Encoding -------------------\ndef pos_enc(x, L=10):\n    freqs = 2.0 ** torch.linspace(0, L-1, L, device=x.device)\n    x = x[..., None] * freqs\n    return torch.cat([torch.sin(x), torch.cos(x)], dim=-1).reshape(*x.shape[:-1], -1)\n\ndef encode_inputs(points, dirs):\n    p_enc = pos_enc(points, L=10)  # (N, 60)\n    d_enc = pos_enc(dirs, L=4)     # (N, 24)\n    return torch.cat([p_enc, d_enc, dirs], dim=-1)  # (N, 87) – adjust input_dim if needed\n\n# ------------------- Volume Rendering -------------------\ndef render_rays(o, d, near=2.0, far=60.0, N=64):\n    t = torch.linspace(near, far, N, device=device)\n    points = o[None, :] + d[None, :] * t[:, None]\n    points_flat = points.reshape(-1, 3)\n    dirs_flat = d.expand(N, 3).reshape(-1, 3)\n    \n    enc = encode_inputs(points_flat, dirs_flat)\n    out = model(enc)\n    rgb = torch.sigmoid(out[:, :3])\n    sigma = F.softplus(out[:, 3])\n    \n    delta = t[1] - t[0]\n    alpha = 1.0 - torch.exp(-sigma * delta)\n    T = torch.cumprod(torch.cat([torch.ones((1,), device=device), 1.0 - alpha + 1e-10], dim=0), dim=0)\n    weights = alpha * T[:-1]\n    \n    rgb_map = torch.sum(weights[:, None] * rgb, dim=0)\n    return rgb_map\n\n# ------------------- Dataset (Your KITTI) -------------------\nclass KITTIDataset(Dataset):\n    def __init__(self, img_dir=\"/kaggle/input/kitti-dataset/data_object_image_2/training/image_2\", \n                 img_wh=512, img_ht=160):\n        self.img_wh, self.img_ht = img_wh, img_ht\n        self.image_paths = sorted([f for f in Path(img_dir).glob(\"*.png\")])\n        self.poses = np.load(\"/kaggle/working/poses.npy\")\n        self.K = np.load(\"/kaggle/working/intrinsics.npy\")\n        \n    def __len__(self): return len(self.image_paths)\n    def __getitem__(self, idx):\n        img = imageio.imread(self.image_paths[idx]) / 255.0\n        img = torch.from_numpy(img).float()\n        img = F.interpolate(img.permute(2,0,1).unsqueeze(0), (self.img_ht, self.img_wh), mode='bilinear').squeeze(0).permute(1,2,0)\n        \n        scale_x = self.img_wh / 1242\n        scale_y = self.img_ht / 375\n        K_scaled = self.K.copy()\n        K_scaled[0] *= scale_x\n        K_scaled[1] *= scale_y\n        \n        pose = torch.from_numpy(self.poses[idx]).float()\n        return img, pose, torch.from_numpy(K_scaled).float()\n\n# ------------------- Training -------------------\ndataset = KITTIDataset()\ndataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n\nprint(\"STARTING TRAINING – 50 epochs = ~1-2 hrs on T4, 300 = full quality\")\nfor epoch in range(1, 51):  # Bump to 301 later\n    total_loss = 0.0\n    pbar = tqdm(dataloader, leave=False)\n    for img, pose, K in pbar:\n        img = img.squeeze(0).to(device)\n        pose = pose.squeeze(0).to(device)\n        K = K.squeeze(0).to(device)\n\n        H, W = img.shape[:2]\n        i, j = torch.meshgrid(torch.arange(W, device=device), torch.arange(H, device=device), indexing='xy')\n        dirs = torch.stack([(i - K[0,2]) / K[0,0], (j - K[1,2]) / K[1,1], -torch.ones_like(i)], -1)\n        dirs = dirs / dirs.norm(dim=-1, keepdim=True)\n        rays_d = (pose[:3,:3] @ dirs.reshape(-1, 3).T).T\n        rays_o = pose[:3,3].expand_as(rays_d)\n\n        idx = torch.randperm(rays_d.shape[0], device=device)[:2048]  # T4-friendly batch\n        rgb_pred = render_rays(rays_o[idx], rays_d[idx])\n        rgb_gt = img.reshape(-1, 3)[idx]\n\n        loss = F.mse_loss(rgb_pred, rgb_gt)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        pbar.set_description(f\"Loss: {loss.item():.6f}\")\n\n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch:3d} – Avg Loss: {total_loss/len(dataloader):.6f}\")\n\nprint(\"Training done. Rendering GIF...\")\n\n# ------------------- Relighting GIF -------------------\nwith torch.no_grad():\n    img, pose, K = dataset[100]\n    img = img.to(device); pose = pose.to(device); K = K.to(device)\n    H, W = img.shape[:2]\n    frames = []\n    for angle in torch.linspace(0, 6.28, 60):\n        light = 0.5 + 0.5 * torch.cos(angle)\n        rgb = torch.zeros(H*W, 3, device=device)\n        batch = 1024\n        for start in range(0, H*W, batch):\n            end = min(start + batch, H*W)\n            i_grid, j_grid = torch.meshgrid(torch.arange(H, device=device), torch.arange(W, device=device), indexing='ij')\n            dirs = torch.stack([(i_grid.flatten()[start:end] - K[0,2]) / K[0,0], (j_grid.flatten()[start:end] - K[1,2]) / K[1,1], -torch.ones(end-start, device=device)], -1)\n            dirs = dirs / dirs.norm(dim=-1, keepdim=True).float()\n            rays_d = (pose[:3,:3] @ dirs.T).T\n            rays_o = pose[:3,3].expand_as(rays_d)\n            rgb[start:end] = render_rays(rays_o, rays_d) * light\n        frame = (255 * rgb.cpu().reshape(H, W, 3).clamp(0,1).numpy()).astype(np.uint8)\n        frames.append(frame)\n\nimageio.mimsave(\"kitti_nerf_final.gif\", frames, fps=15)\ndisplay(Image(\"kitti_nerf_final.gif\"))\nprint(\"DONE – REAL KITTI NeRF GIF READY FOR GITHUB!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T12:20:27.080906Z","iopub.execute_input":"2025-12-07T12:20:27.081183Z","iopub.status.idle":"2025-12-07T12:20:29.002227Z","shell.execute_reply.started":"2025-12-07T12:20:27.081157Z","shell.execute_reply":"2025-12-07T12:20:29.001330Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nModel created successfully!\nSTARTING TRAINING – 50 epochs = ~1-2 hrs on T4, 300 = full quality\n","output_type":"stream"},{"name":"stderr","text":"                                        \r","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/76766874.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrays_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# T4-friendly batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mrgb_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrender_rays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrays_o\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrays_d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mrgb_gt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/76766874.py\u001b[0m in \u001b[0;36mrender_rays\u001b[0;34m(o, d, near, far, N)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrender_rays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mpoints_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mdirs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2048) must match the size of tensor b (64) at non-singleton dimension 1"],"ename":"RuntimeError","evalue":"The size of tensor a (2048) must match the size of tensor b (64) at non-singleton dimension 1","output_type":"error"}],"execution_count":28},{"cell_type":"code","source":"import imageio.v2 as imageio\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nfrom pathlib import Path\nfrom IPython.display import Image\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ------------------- Vanilla PyTorch NeRF MLP -------------------\nclass NeRFMLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        layers = [nn.Linear(63, 256), nn.ReLU()]\n        for _ in range(7):\n            layers += [nn.Linear(256, 256), nn.ReLU()]\n        layers += [nn.Linear(256, 4)]\n        self.net = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        return self.net(x)\n\nmodel = NeRFMLP().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n\n# ------------------- Positional Encoding -------------------\ndef pos_enc(x, L=10):\n    freqs = 2.0 ** torch.linspace(0, L-1, L, device=x.device)\n    x = x[..., None] * freqs\n    return torch.cat([torch.sin(x), torch.cos(x)], dim=-1).reshape(*x.shape[:-1], -1)\n\ndef encode_inputs(points, dirs):\n    return torch.cat([pos_enc(points, 10), pos_enc(dirs, 4), dirs], dim=-1)\n\n# ------------------- Volume Rendering (FIXED) -------------------\ndef render_rays(o, d, near=2.0, far=60.0, N_samples=64):\n    # o: (R, 3), d: (R, 3)\n    t = torch.linspace(near, far, N_samples, device=device)\n    points = o[:, None, :] + d[:, None, :] * t[None, :, None]   # (R, N, 3)\n    dirs_rep = d[:, None, :].expand(-1, N_samples, -1)\n    \n    points_flat = points.reshape(-1, 3)\n    dirs_flat = dirs_rep.reshape(-1, 3)\n    \n    enc = encode_inputs(points_flat, dirs_flat)\n    out = model(enc)\n    rgb = torch.sigmoid(out[:, :3])\n    sigma = F.softplus(out[:, 3])\n    \n    delta = t[1] - t[0]\n    alpha = 1.0 - torch.exp(-sigma * delta)\n    T = torch.cumprod(torch.cat([torch.ones(alpha.shape[0], 1, device=device), 1.0 - alpha + 1e-10], dim=1), dim=1)\n    weights = alpha * T[:, :-1]\n    \n    rgb_map = torch.sum(weights[:, None] * rgb.reshape(-1, N_samples, 3), dim=1)\n    return rgb_map\n\n# ------------------- Dataset (Your KITTI) -------------------\nclass KITTIDataset(Dataset):\n    def __init__(self, img_dir=\"/kaggle/input/kitti-dataset/data_object_image_2/training/image_2\", \n                 img_wh=512, img_ht=160):\n        self.img_wh, self.img_ht = img_wh, img_ht\n        self.image_paths = sorted(Path(img_dir).glob(\"*.png\"))\n        self.poses = np.load(\"/kaggle/working/poses.npy\")\n        self.K = np.load(\"/kaggle/working/intrinsics.npy\")\n        \n    def __len__(self): return len(self.image_paths)\n    def __getitem__(self, idx):\n        img = imageio.imread(self.image_paths[idx]) / 255.0\n        img = torch.from_numpy(img).float()\n        img = F.interpolate(img.permute(2,0,1).unsqueeze(0), (self.img_ht, self.img_wh), mode='bilinear').squeeze(0).permute(1,2,0)\n        \n        scale_x = self.img_wh / 1242.0\n        scale_y = self.img_ht / 375.0\n        K_scaled = self.K.copy().astype(np.float32)\n        K_scaled[0] *= scale_x\n        K_scaled[1] *= scale_y\n        \n        pose = torch.from_numpy(self.poses[idx]).float()\n        return img, pose, torch.from_numpy(K_scaled).float()\n\ndataset = KITTIDataset()\ndataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n\n# ------------------- TRAINING LOOP (NOW 100% WORKING) -------------------\nprint(\"STARTING TRAINING – 50 epochs = 1-2 hrs\")\nfor epoch in range(1, 51):\n    total_loss = 0.0\n    for img, pose, K in tqdm(dataloader, desc=f\"Epoch {epoch}\", leave=False):\n        img = img.squeeze(0).to(device)\n        pose = pose.squeeze(0).to(device)\n        K = K.squeeze(0).to(device)\n\n        H, W = img.shape[:2]\n        i, j = torch.meshgrid(torch.arange(W, device=device), torch.arange(H, device=device), indexing='xy')\n        dirs = torch.stack([(i - K[0,2]) / K[0,0], (j - K[1,2]) / K[1,1], -torch.ones_like(i)], -1)\n        dirs = F.normalize(dirs, dim=-1)\n        rays_d = (pose[:3,:3] @ dirs.reshape(-1, 3).T).T\n        rays_o = pose[:3,3].expand_as(rays_d)\n\n        # Sample 2048 random rays\n        idx = torch.randperm(rays_d.shape[0], device=device)[:2048]\n        rgb_pred = render_rays(rays_o[idx], rays_d[idx])\n        rgb_gt = img.reshape(-1, 3)[idx]\n\n        loss = F.mse_loss(rgb_pred, rgb_gt)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch:3d} – Loss: {total_loss/len(dataloader):.6f}\")\n\nprint(\"Training finished! Rendering GIF...\")\n\n# ------------------- RELIGHTING GIF -------------------\nwith torch.no_grad():\n    img, pose, K = dataset[200]\n    img = img.to(device); pose = pose.to(device); K = K.to(device)\n    H, W = img.shape[:2]\n    frames = []\n    for angle in torch.linspace(0, 6.28, 60):\n        light = 0.5 + 0.5 * torch.cos(angle)\n        rgb_full = torch.zeros(H*W, 3, device=device)\n        batch = 2048\n        for i in range(0, H*W, batch):\n            end = min(i + batch, H*W)\n            ii, jj = torch.meshgrid(torch.arange(H, device=device), torch.arange(W, device=device), indexing='ij')\n            dirs = torch.stack([(ii - K[0,2]) / K[0,0], (jj - K[1,2]) / K[1,1], -torch.ones_like(ii)], -1).reshape(-1, 3)\n            dirs = F.normalize(dirs, dim=-1)\n            rays_d = (pose[:3,:3] @ dirs.T).T\n            rays_o = pose[:3,3].expand_as(rays_d)\n            rgb_full[i:end] = render_rays(rays_o[i:end], rays_d[i:end1:end]) * light\n        frame = (255 * rgb_full.cpu().reshape(H, W, 3).clamp(0,1).numpy()).astype(np.uint8)\n        frames.append(frame)\n\nimageio.mimsave(\"kitti_nerf_final.gif\", frames, fps=15)\ndisplay(Image(\"kitti_nerf_final.gif\"))\nprint(\"KITTI NeRF COMPLETE – PUSH TO GITHUB NOW!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T12:21:39.536658Z","iopub.execute_input":"2025-12-07T12:21:39.537096Z","iopub.status.idle":"2025-12-07T12:21:39.725980Z","shell.execute_reply.started":"2025-12-07T12:21:39.537072Z","shell.execute_reply":"2025-12-07T12:21:39.725104Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nSTARTING TRAINING – 50 epochs = 1-2 hrs\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/3017454765.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# Sample 2048 random rays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrays_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mrgb_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrender_rays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrays_o\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrays_d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mrgb_gt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/3017454765.py\u001b[0m in \u001b[0;36mrender_rays\u001b[0;34m(o, d, near, far, N_samples)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mdirs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirs_rep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mrgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/3017454765.py\u001b[0m in \u001b[0;36mencode_inputs\u001b[0;34m(points, dirs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mencode_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_enc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_enc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# ------------------- Volume Rendering (FIXED) -------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 2"],"ename":"RuntimeError","evalue":"Tensors must have same number of dimensions: got 3 and 2","output_type":"error"}],"execution_count":31},{"cell_type":"code","source":"import imageio.v2 as imageio\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nfrom pathlib import Path\nfrom IPython.display import Image\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ------------------- Vanilla PyTorch NeRF MLP -------------------\nclass NeRFMLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        layers = [nn.Linear(63, 256), nn.ReLU()]\n        for _ in range(7):\n            layers += [nn.Linear(256, 256), nn.ReLU()]\n        layers += [nn.Linear(256, 4)]\n        self.net = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        return self.net(x)\n\nmodel = NeRFMLP().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T12:22:56.451138Z","iopub.execute_input":"2025-12-07T12:22:56.451862Z","iopub.status.idle":"2025-12-07T12:22:56.465064Z","shell.execute_reply.started":"2025-12-07T12:22:56.451836Z","shell.execute_reply":"2025-12-07T12:22:56.464456Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# ------------------- Positional Encoding -------------------\ndef pos_enc(x, L=10):\n    freqs = 2.0 ** torch.linspace(0, L-1, L, device=x.device)\n    x = x[..., None] * freqs\n    return torch.cat([torch.sin(x), torch.cos(x)], dim=-1).reshape(*x.shape[:-1], -1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T12:23:22.426109Z","iopub.execute_input":"2025-12-07T12:23:22.426412Z","iopub.status.idle":"2025-12-07T12:23:22.430908Z","shell.execute_reply.started":"2025-12-07T12:23:22.426389Z","shell.execute_reply":"2025-12-07T12:23:22.430294Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def encode_inputs(points, dirs):\n    p_enc = pos_enc(points, L=10)   # (R*N, 60)\n    d_enc = pos_enc(dirs, L=4)      # (R*N, 24)\n    return torch.cat([p_enc, d_enc, dirs], dim=-1)   # (R*N, 87)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T12:37:34.767790Z","iopub.execute_input":"2025-12-07T12:37:34.768277Z","iopub.status.idle":"2025-12-07T12:37:34.772296Z","shell.execute_reply.started":"2025-12-07T12:37:34.768238Z","shell.execute_reply":"2025-12-07T12:37:34.771503Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"def render_rays(o, d, near=2.0, far=60.0, N_samples=64):\n    # o: (R, 3), d: (R, 3)\n    R = o.shape[0]\n    t = torch.linspace(near, far, N_samples, device=device)\n    \n    # (R, N_samples, 3)\n    points = o[:, None, :] + d[:, None, :] * t[None, :, None]\n    dirs_rep = d[:, None, :].expand(-1, N_samples, -1)\n    \n    # Flatten\n    points_flat = points.reshape(-1, 3)      # (R*N, 3)\n    dirs_flat = dirs_rep.reshape(-1, 3)      # (R*N, 3)\n    \n    # Encode\n    enc = encode_inputs(points_flat, dirs_flat)   # (R*N, 87)\n    out = model(enc)\n    rgb = torch.sigmoid(out[:, :3])\n    sigma = F.softplus(out[:, 3:])\n    \n    # Volume rendering\n    delta = t[1] - t[0]\n    alpha = 1.0 - torch.exp(-sigma * delta)\n    T = torch.cumprod(torch.cat([torch.ones((R*N, 1), device=device), 1.0 - alpha + 1e-10], dim=1), dim=1)\n    weights = alpha * T[:, :-1]\n    \n    rgb_map = torch.sum(weights * rgb.reshape(R*N, N_samples, 3), dim=1)\n    return rgb_map","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T12:37:11.220482Z","iopub.execute_input":"2025-12-07T12:37:11.221025Z","iopub.status.idle":"2025-12-07T12:37:11.227156Z","shell.execute_reply.started":"2025-12-07T12:37:11.221001Z","shell.execute_reply":"2025-12-07T12:37:11.226312Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# ------------------- Dataset (Your KITTI) -------------------\nclass KITTIDataset(Dataset):\n    def __init__(self, img_dir=\"/kaggle/input/kitti-dataset/data_object_image_2/training/image_2\", \n                 img_wh=512, img_ht=160):\n        self.img_wh, self.img_ht = img_wh, img_ht\n        self.image_paths = sorted(Path(img_dir).glob(\"*.png\"))\n        self.poses = np.load(\"/kaggle/working/poses.npy\")\n        self.K = np.load(\"/kaggle/working/intrinsics.npy\")\n        \n    def __len__(self): return len(self.image_paths)\n    def __getitem__(self, idx):\n        img = imageio.imread(self.image_paths[idx]) / 255.0\n        img = torch.from_numpy(img).float()\n        img = F.interpolate(img.permute(2,0,1).unsqueeze(0), (self.img_ht, self.img_wh), mode='bilinear').squeeze(0).permute(1,2,0)\n        \n        scale_x = self.img_wh / 1242.0\n        scale_y = self.img_ht / 375.0\n        K_scaled = self.K.copy().astype(np.float32)\n        K_scaled[0] *= scale_x\n        K_scaled[1] *= scale_y\n        \n        pose = torch.from_numpy(self.poses[idx]).float()\n        return img, pose, torch.from_numpy(K_scaled).float()\n\ndataset = KITTIDataset()\ndataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T12:37:38.531427Z","iopub.execute_input":"2025-12-07T12:37:38.531706Z","iopub.status.idle":"2025-12-07T12:37:38.585232Z","shell.execute_reply.started":"2025-12-07T12:37:38.531686Z","shell.execute_reply":"2025-12-07T12:37:38.584720Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# ------------------- TRAINING LOOP (NOW 100% WORKING) -------------------\nprint(\"STARTING TRAINING – 50 epochs = 1-2 hrs\")\nfor epoch in range(1, 51):\n    total_loss = 0.0\n    for img, pose, K in tqdm(dataloader, desc=f\"Epoch {epoch}\", leave=False):\n        img = img.squeeze(0).to(device)\n        pose = pose.squeeze(0).to(device)\n        K = K.squeeze(0).to(device)\n\n        H, W = img.shape[:2]\n        i, j = torch.meshgrid(torch.arange(W, device=device), torch.arange(H, device=device), indexing='xy')\n        dirs = torch.stack([(i - K[0,2]) / K[0,0], (j - K[1,2]) / K[1,1], -torch.ones_like(i)], -1)\n        dirs = F.normalize(dirs, dim=-1)\n        rays_d = (pose[:3,:3] @ dirs.reshape(-1, 3).T).T\n        rays_o = pose[:3,3].expand_as(rays_d)\n\n        # Sample 2048 random rays\n        idx = torch.randperm(rays_d.shape[0], device=device)[:2048]\n        rgb_pred = render_rays(rays_o[idx], rays_d[idx])\n        rgb_gt = img.reshape(-1, 3)[idx]\n\n        loss = F.mse_loss(rgb_pred, rgb_gt)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch:3d} – Loss: {total_loss/len(dataloader):.6f}\")\n\nprint(\"Training finished! Rendering GIF...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T12:37:48.188887Z","iopub.execute_input":"2025-12-07T12:37:48.189139Z","iopub.status.idle":"2025-12-07T12:37:48.255909Z","shell.execute_reply.started":"2025-12-07T12:37:48.189120Z","shell.execute_reply":"2025-12-07T12:37:48.254983Z"}},"outputs":[{"name":"stdout","text":"STARTING TRAINING – 50 epochs = 1-2 hrs\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2319084519.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Sample 2048 random rays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrays_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mrgb_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrender_rays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrays_o\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrays_d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mrgb_gt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/3422655355.py\u001b[0m in \u001b[0;36mrender_rays\u001b[0;34m(o, d, near, far, N_samples)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Encode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs_flat\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (R*N, 87)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mrgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/2154137306.py\u001b[0m in \u001b[0;36mencode_inputs\u001b[0;34m(points, dirs)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mp_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_enc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (R*N, 60)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0md_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_enc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# (R*N, 24)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (R*N, 87)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 2"],"ename":"RuntimeError","evalue":"Tensors must have same number of dimensions: got 3 and 2","output_type":"error"}],"execution_count":42},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\n# DEBUG: Let's see what shapes we actually get\ndef debug_shapes(o, d):\n    print(f\"rays_o shape: {o.shape}, rays_d shape: {d.shape}\")\n\n# FINAL FIXED pos_enc\ndef pos_enc(x, L=10):\n    freqs = 2.0 ** torch.linspace(0, L-1, L, device=x.device)\n    x = x[..., None] * freqs\n    return torch.cat([torch.sin(x), torch.cos(x)], dim=-1).reshape(*x.shape[:-1], -1)\n\n# FINAL FIXED encode_inputs\ndef encode_inputs(points, dirs):\n    # points: (R*N, 3), dirs: (R*N, 3)\n    p_enc = pos_enc(points, L=10)   # → (R*N, 60)\n    d_enc = pos_enc(dirs, L=4)      # → (R*N, 24)\n    return torch.cat([p_enc, d_enc, dirs], dim=-1)  # → (R*N, 87)\n\n# FINAL FIXED render_rays\ndef render_rays(o, d, near=2.0, far=60.0, N_samples=64):\n    R = o.shape[0]\n    t = torch.linspace(near, far, N_samples, device=o.device)\n    \n    # (R, N_samples, 3)\n    points = o[:, None, :] + d[:, None, :] * t[None, :, None]\n    dirs_rep = d[:, None, :].expand(R, N_samples, 3)\n    \n    # Flatten to (R*N, 3)\n    points_flat = points.reshape(-1, 3)\n    dirs_flat = dirs_rep.reshape(-1, 3)\n    \n    enc = encode_inputs(points_flat, dirs_flat)   # (R*N, 87)\n    out = model(enc)\n    rgb = torch.sigmoid(out[:, :3])\n    sigma = F.softplus(out[:, 3:]).squeeze(-1)   # (R*N,)\n    \n    delta = t[1] - t[0]\n    alpha = 1.0 - torch.exp(-sigma * delta)\n    T = torch.cumprod(torch.cat([torch.ones(R*N, 1, device=o.device), 1.0 - alpha[:, None] + 1e-10], dim=1), dim=1)\n    weights = alpha[:, None] * T[:, :-1]\n    \n    rgb_map = torch.sum(weights * rgb.reshape(R*N, N_samples, 3), dim=1)\n    return rgb_map\n\nprint(\"All functions fixed and ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T12:39:56.032464Z","iopub.execute_input":"2025-12-07T12:39:56.032745Z","iopub.status.idle":"2025-12-07T12:39:56.041982Z","shell.execute_reply.started":"2025-12-07T12:39:56.032725Z","shell.execute_reply":"2025-12-07T12:39:56.041416Z"}},"outputs":[{"name":"stdout","text":"All functions fixed and ready!\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"# Test one image to confirm everything works\nimg, pose, K = dataset[0]\nimg = img.to(device)\npose = pose.to(device)\nK = K.to(device)\n\nH, W = img.shape[:2]\ni, j = torch.meshgrid(torch.arange(W, device=device), torch.arange(H, device=device), indexing='xy')\ndirs = torch.stack([(i - K[0,2]) / K[0,0], (j - K[1,2]) / K[1,1], -torch.ones_like(i)], -1)\ndirs = F.normalize(dirs, dim=-1)\nrays_d = (pose[:3,:3] @ dirs.reshape(-1, 3).T).T\nrays_o = pose[:3,3].expand_as(rays_d)\n\nidx = torch.randperm(rays_d.shape[0], device=device)[:2048]\nrgb_pred = render_rays(rays_o[idx], rays_d[idx])\nprint(f\"rgb_pred shape: {rgb_pred.shape} - SUCCESS!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T12:40:31.099989Z","iopub.execute_input":"2025-12-07T12:40:31.100745Z","iopub.status.idle":"2025-12-07T12:40:31.151146Z","shell.execute_reply.started":"2025-12-07T12:40:31.100719Z","shell.execute_reply":"2025-12-07T12:40:31.150191Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/3582624587.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrays_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mrgb_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrender_rays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrays_o\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrays_d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"rgb_pred shape: {rgb_pred.shape} - SUCCESS!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/1091333627.py\u001b[0m in \u001b[0;36mrender_rays\u001b[0;34m(o, d, near, far, N_samples)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mdirs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirs_rep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs_flat\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (R*N, 87)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mrgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/1091333627.py\u001b[0m in \u001b[0;36mencode_inputs\u001b[0;34m(points, dirs)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mp_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_enc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# → (R*N, 60)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0md_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_enc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# → (R*N, 24)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# → (R*N, 87)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# FINAL FIXED render_rays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 2"],"ename":"RuntimeError","evalue":"Tensors must have same number of dimensions: got 3 and 2","output_type":"error"}],"execution_count":45},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\n# FIXED pos_enc - correct reshape\ndef pos_enc(x, L=10):\n    # x: (R*N, 3)\n    freqs = 2.0 ** torch.linspace(0, L-1, L, device=x.device)\n    x_expanded = x[..., None] * freqs  # (R*N, 3, L)\n    encoded = torch.cat([torch.sin(x_expanded), torch.cos(x_expanded)], dim=-1)  # (R*N, 3, 2*L)\n    # Flatten the last two dimensions: (R*N, 3*2*L)\n    return encoded.reshape(encoded.shape[0], -1)\n\n# FIXED encode_inputs - match model's expected input size (63)\ndef encode_inputs(points, dirs):\n    # points: (R*N, 3), dirs: (R*N, 3)\n    p_enc = pos_enc(points, L=10)   # → (R*N, 60)\n    # Only use raw directions, no positional encoding\n    # This gives us 60 + 3 = 63 features\n    return torch.cat([p_enc, dirs], dim=-1)  # → (R*N, 63)\n\n# FIXED render_rays\ndef render_rays(o, d, near=2.0, far=6.0, N_samples=64):\n    R = o.shape[0]\n    t = torch.linspace(near, far, N_samples, device=o.device)\n    \n    # (R, N_samples, 3)\n    points = o[:, None, :] + d[:, None, :] * t[None, :, None]\n    dirs_rep = d[:, None, :].expand(R, N_samples, 3)\n    \n    # Flatten to (R*N, 3)\n    points_flat = points.reshape(-1, 3)\n    dirs_flat = dirs_rep.reshape(-1, 3)\n    \n    enc = encode_inputs(points_flat, dirs_flat)   # (R*N, 63)\n    out = model(enc)\n    rgb = torch.sigmoid(out[:, :3])\n    sigma = F.softplus(out[:, 3:]).squeeze(-1)   # (R*N,)\n    \n    # Reshape back to (R, N_samples) for volume rendering\n    rgb = rgb.reshape(R, N_samples, 3)\n    sigma = sigma.reshape(R, N_samples)\n    \n    delta = t[1] - t[0]\n    alpha = 1.0 - torch.exp(-sigma * delta)  # (R, N_samples)\n    \n    # Compute transmittance\n    T = torch.cumprod(torch.cat([torch.ones(R, 1, device=o.device), 1.0 - alpha + 1e-10], dim=1), dim=1)[:, :-1]  # (R, N_samples)\n    \n    weights = alpha * T  # (R, N_samples)\n    \n    rgb_map = torch.sum(weights[..., None] * rgb, dim=1)  # (R, 3)\n    return rgb_map\n\nprint(\"All functions fixed and ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T12:50:11.663947Z","iopub.execute_input":"2025-12-07T12:50:11.664617Z","iopub.status.idle":"2025-12-07T12:50:11.674628Z","shell.execute_reply.started":"2025-12-07T12:50:11.664591Z","shell.execute_reply":"2025-12-07T12:50:11.673872Z"}},"outputs":[{"name":"stdout","text":"All functions fixed and ready!\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"# Test one image to confirm everything works\nimg, pose, K = dataset[0]\nimg = img.to(device)\npose = pose.to(device)\nK = K.to(device)\n\nH, W = img.shape[:2]\ni, j = torch.meshgrid(torch.arange(W, device=device), torch.arange(H, device=device), indexing='xy')\ndirs = torch.stack([(i - K[0,2]) / K[0,0], (j - K[1,2]) / K[1,1], -torch.ones_like(i)], -1)\ndirs = F.normalize(dirs, dim=-1)\nrays_d = (pose[:3,:3] @ dirs.reshape(-1, 3).T).T\nrays_o = pose[:3,3].expand_as(rays_d)\n\nidx = torch.randperm(rays_d.shape[0], device=device)[:2048]\nrgb_pred = render_rays(rays_o[idx], rays_d[idx])\nprint(f\"rgb_pred shape: {rgb_pred.shape} - SUCCESS!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T12:50:14.805687Z","iopub.execute_input":"2025-12-07T12:50:14.805946Z","iopub.status.idle":"2025-12-07T12:50:14.975319Z","shell.execute_reply.started":"2025-12-07T12:50:14.805927Z","shell.execute_reply":"2025-12-07T12:50:14.974545Z"}},"outputs":[{"name":"stdout","text":"rgb_pred shape: torch.Size([2048, 3]) - SUCCESS!\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"# ------------------- IMPROVED TRAINING LOOP -------------------\nimport time\n\nprint(\"STARTING TRAINING – 50 epochs\")\nbest_loss = float('inf')\nstart_time = time.time()\n\nfor epoch in range(1, 51):\n    model.train()  # Ensure model is in training mode\n    total_loss = 0.0\n    num_batches = 0\n    \n    for img, pose, K in tqdm(dataloader, desc=f\"Epoch {epoch}\", leave=False):\n        img = img.squeeze(0).to(device)\n        pose = pose.squeeze(0).to(device)\n        K = K.squeeze(0).to(device)\n        H, W = img.shape[:2]\n        \n        # Generate rays (this could be cached if poses don't change)\n        i, j = torch.meshgrid(torch.arange(W, device=device), torch.arange(H, device=device), indexing='xy')\n        dirs = torch.stack([(i - K[0,2]) / K[0,0], (j - K[1,2]) / K[1,1], -torch.ones_like(i)], -1)\n        dirs = F.normalize(dirs, dim=-1)\n        rays_d = (pose[:3,:3] @ dirs.reshape(-1, 3).T).T\n        rays_o = pose[:3,3].expand_as(rays_d)\n        \n        # Sample 2048 random rays\n        idx = torch.randperm(rays_d.shape[0], device=device)[:2048]\n        rgb_pred = render_rays(rays_o[idx], rays_d[idx])\n        rgb_gt = img.reshape(-1, 3)[idx]\n        \n        loss = F.mse_loss(rgb_pred, rgb_gt)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n        optimizer.step()\n        \n        total_loss += loss.item()\n        num_batches += 1\n    \n    avg_loss = total_loss / num_batches\n    \n    # Learning rate scheduling\n    if epoch % 10 == 0:\n        for param_group in optimizer.param_groups:\n            param_group['lr'] *= 0.5\n            print(f\"  → Learning rate reduced to {param_group['lr']:.6f}\")\n    \n    # Save best model\n    if avg_loss < best_loss:\n        best_loss = avg_loss\n        torch.save(model.state_dict(), 'best_nerf_model.pth')\n    \n    # Progress reporting\n    elapsed = time.time() - start_time\n    eta = elapsed / epoch * (50 - epoch)\n    print(f\"Epoch {epoch:3d} – Loss: {avg_loss:.6f} | Best: {best_loss:.6f} | \"\n          f\"Time: {elapsed/60:.1f}m | ETA: {eta/60:.1f}m\")\n    \n    # Periodic checkpoint\n    if epoch % 2 == 0:\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': avg_loss,\n        }, f'checkpoint_epoch_{epoch}.pth')\n\nprint(f\"\\nTraining finished in {(time.time()-start_time)/60:.1f} minutes!\")\nprint(f\"Best loss: {best_loss:.6f}\")\nprint(\"Loading best model for rendering...\")\nmodel.load_state_dict(torch.load('best_nerf_model.pth'))\nmodel.eval()\nprint(\"Rendering GIF...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T14:38:52.451846Z","iopub.execute_input":"2025-12-07T14:38:52.452364Z"}},"outputs":[{"name":"stdout","text":"STARTING TRAINING – 50 epochs\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch   1 – Loss: 0.087877 | Best: 0.087877 | Time: 18.7m | ETA: 918.5m\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch   2 – Loss: 0.087854 | Best: 0.087854 | Time: 37.3m | ETA: 896.0m\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3:  34%|███▍      | 2548/7481 [06:11<11:55,  6.89it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# After downloading a checkpoint (e.g., checkpoint_epoch_20.pth)\nresume_training_from_checkpoint(\n    checkpoint_path='checkpoint_epoch_20.pth',  # Your downloaded checkpoint\n    model=model,\n    optimizer=optimizer,\n    dataloader=dataloader,\n    device=device,\n    total_epochs=50,  # Train until epoch 50 (not 30 additional epochs)\n    save_interval=10\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# After training completes or loading best model\nrun_full_inference(\n    model_path='best_nerf_model.pth',\n    dataset=dataset,\n    device=device\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.load_state_dict(torch.load('best_nerf_model.pth'))\n_, _, K = dataset[0]\ncreate_360_video(\n    model=model,\n    K=K.to(device),\n    H=400,\n    W=400,\n    device=device,\n    output_path='nerf_360.gif',\n    n_frames=120,\n    radius=4.0,\n    fps=30\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.load_state_dict(torch.load('best_nerf_model.pth'))\nmetrics = evaluate_model(model, dataset, device, num_test_views=10)\nprint(f\"Average PSNR: {metrics['psnr']:.2f} dB\")\nvisualize_results(metrics)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# You have completed epoch 2, so your checkpoint is:\nresume_training_from_checkpoint(\n    checkpoint_path='checkpoint_epoch_2.pth',  # Your checkpoint after 2 epochs\n    model=model,\n    optimizer=optimizer,\n    dataloader=dataloader,\n    device=device,\n    total_epochs=50,  # This means \"train UNTIL epoch 50\"\n    save_interval=10\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}